{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Natural Language Processing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Install NLTK - pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 0 - Get some Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import urllib\n",
    "import bs4 as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# We will read the contents of the Wikipedia article \"Global_warming\" as an example, please feel free to use your own!\n",
    "# We can open the page using \"urllib.request.urlopen\" then read it using \".read()\"\n",
    "source = urllib.request.urlopen('[insert your url here]').read()\n",
    "\n",
    "# Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "# you may need to install a parser library --> \"!pip3 install lxml\"\n",
    "# Parsing the data/creating BeautifulSoup object\n",
    "\n",
    "soup = bs.BeautifulSoup(source,\"html.parser\") \n",
    "\n",
    "# Fetching the data\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n",
    "    text += paragraph.text\n",
    "\n",
    "# Preprocessing the data\n",
    "\n",
    "    ###Use regex to clean the data##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 1 - Tokenization of paragraphs/sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ###Tokenizing sentences###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ###Tokenizing words##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 2 - Stopwords and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "    ### define a function called \"remove_stopwords\" that takes in a list of the sentences of the text and returns one that doesn't have any stopwords##\n",
    "def remove_stopwords(sentences):\n",
    "    \n",
    "    ### Some code goes here###\n",
    "    \n",
    "    return sentences\n",
    "    ###Then actually apply your function###\n",
    "sentences = remove_stopwords(sentences)\n",
    "print(sentences[:10]) #eliminating all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ##Do the same for removing punctuation\n",
    "def remove_punctuation(sentences):\n",
    "    \n",
    "    ### Some code goes here###\n",
    "    \n",
    "    return sentences\n",
    "sentences = remove_punctuation(sentences)\n",
    "print(sentences[:10]) #eliminating all punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 3a - Stemming the words\n",
    "#### stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# try each of the words below\n",
    "stemmer.stem('troubled')\n",
    "#stemmer.stem('trouble')\n",
    "#stemmer.stem('troubling')\n",
    "#stemmer.stem('troubles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ### Stemming ###\n",
    "##Similar to stopwords: For loop through the sentences, split by words and apply \"stemmer.stem\" to each word and join back into a sentence\n",
    "def stem_sentences(sentences):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n",
    "print(stemmed_sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NLP Part 3b - Lemmatization\n",
    "#### Lemmatization considers the context and converts the word to its meaningful base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    ##Import the lemmatizer##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    ## Lemmatization ##\n",
    "##Similar to stopwords: For loop through the sentences, split by words and apply \"lemmatizer.lemmatize\" to each word and join back into a sentence\n",
    "def lem_sentences(sentences):\n",
    "\n",
    "    return sentences\n",
    "sentences = lem_sentences(sentences)\n",
    "print(sentences[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(sentences[:10]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### some other prepossessing methods using NLTK\n",
    "## NLP Part 4 - POS Tagging\n",
    "#### marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# POS Tagging\n",
    "# example\n",
    "# CC - coordinating conjunction\n",
    "# NN - noun, singular (cat, tree)\n",
    "\n",
    "# Tagged word paragraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word2Vec Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Install gensim - pip install gensim\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "#Let's go ahead and create a list that's formatted how word2vec needs:\n",
    "    # a list of lists where the ith entry in the list is the word tokenizaiton of the ith sentence (after preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# Training the Word2Vec model\n",
    "\n",
    "\n",
    "\n",
    "# save the model to use it later\n",
    "\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "most_common_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    # Finding Word Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ### Finding the most similar words in the model ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "similar1, similar2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Why did we do all this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# reFetching the data\n",
    "lame_text = \"\"\n",
    "for paragraph in soup.find_all('p'): #The <p> tag defines a paragraph in the webpages\n",
    "    lame_text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ###Doing the same without removing stop words or lemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    #Redoing the word cloud, but badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    # Training the Word2Vec model, but badly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ### Finding a vector of a word, but badly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "    ### Finding the most similar words in the modelm but... you get the idea ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CPU Kernel",
   "language": "python",
   "name": "myenv",
   "resource_dir": "/projects/9d5fc2b3-70ee-401f-b938-559ac8a15479/.local/share/jupyter/kernels/myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}